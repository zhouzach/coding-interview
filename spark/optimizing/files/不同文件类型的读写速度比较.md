https://dzone.com/articles/tips-and-best-practices-to-take-advantage-of-spark

File data stores are good for write once (append only), read many use cases. CSV and JSON data formats give excellent 
write path performance but are slower for reading; these formats are good candidates for collecting raw data for example logs, 
which require high throughput writes. Parquet is slower for writing but gives the best performance for reading; 
this format is good for BI and analytics, which require low latency reads.

When persisting and compressing CSV and JSON files, make sure they are splittable, give high speeds, and yield reasonable 
compression. ZIP compression is not splittable, whereas Snappy is splittable; Snappy also gives reasonable compression 
with high speed. When reading CSV and JSON files, you will get better performance by specifying the schema instead of 
using inference; specifying the schema reduces errors for data types and is recommended for production code. 
See chapter 2 in the eBook for examples of specifying the schema on read.