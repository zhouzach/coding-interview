
Container killed by YARN for exceeding memory limits. 9.0 GB of 9 GB physical memory used.
Consider boosting spark.yarn.executor.memoryOverhead.

https://stackoverflow.com/questions/31840492/how-to-avoid-spark-executor-from-getting-lost-and-yarn-container-killing-it-due

--conf spark.yarn.executor.memoryOverhead=1024
 this is set in MB.
 Yarn kills executors when its memory usage is larger then (executor-memory + executor.memoryOverhead)


 https://gsamaras.wordpress.com/code/memoryoverhead-issue-in-spark/
